{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2536/2536 [00:11<00:00, 228.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2100/2100 [00:18<00:00, 112.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import js_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import esprima\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import feature_extraction\n",
    "from feature_extraction import tfidf_extractor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# 0 - benign, 1 - malicious\n",
    "\n",
    "\n",
    "def extract_script_from_html(html: str):\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    scripts = []\n",
    "    for script in soup.find_all(\"script\"):\n",
    "        scripts.append(script)\n",
    "    return scripts\n",
    "\n",
    "def tokenize_html(html: str):\n",
    "    scripts_tokenize = ''\n",
    "    for script in extract_script_from_html(html):\n",
    "        scripts_tokenize += esprima.tokenize(script)\n",
    "    return scripts_tokenize\n",
    "\n",
    "def tokenize(file_path: str, js_file=True):\n",
    "    try:\n",
    "        with open(file_path, \"r\", errors='surrogateescape') as f:\n",
    "            if js_file:\n",
    "                return esprima.tokenize(f.read())\n",
    "            else:\n",
    "                return tokenize_html(f.read())\n",
    "    except:\n",
    "        return []\n",
    "paths_benign = []\n",
    "for s, d, f in os.walk(r\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\BENIGN\"):\n",
    "    for file in f:\n",
    "        paths_benign.append(os.path.join(s,file))\n",
    "paths_mal = []\n",
    "for s, d, f in os.walk(r\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\MALICIOUS\"):\n",
    "    for file in f:\n",
    "        paths_mal.append(os.path.join(s,file))\n",
    "print(2)\n",
    "mal_tokenize = []\n",
    "for path in tqdm(paths_mal):\n",
    "    mal_tokenize.append(tokenize(path,path.endswith('.js')))\n",
    "\n",
    "benign_tokenize = []\n",
    "for path in tqdm(paths_benign):\n",
    "    benign_tokenize.append(tokenize(path,path.endswith('.js')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_flat = []\n",
    "for item in mal_tokenize:\n",
    "    mal_flat.append(\" \".join([\"{0}_{1}\".format(token.type,token.value.replace(' ','-')) for token in item]))\n",
    "benign_flat = []\n",
    "for item in benign_tokenize:\n",
    "    benign_flat.append(\" \".join([\"{0}_{1}\".format(token.type,token.value.replace(' ','-')) for token in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(mal_flat)\n",
    "malicious_tfidf = vectorizer.transform(mal_flat)\n",
    "benign_tfidf = vectorizer.transform(benign_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Preparing Train-Test data\n",
      "[+] 5 Fold Cross Validation Random Forest Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluating model\n",
      "\t- Accuracy = 0.8888935386675595\n",
      "\t- Precision = 0.9202631850152698\n",
      "\t- Recall = 0.8560313096957556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edenm\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "num_benign_samples = benign_tfidf.shape[0]\n",
    "num_malicious_samples = malicious_tfidf.shape[0]\n",
    "\n",
    "# 0 - benign, 1 - malicious\n",
    "benign_classifications = [0 for _ in range(num_benign_samples)]\n",
    "malicious_classifications = [1 for _ in range(num_malicious_samples)]\n",
    "\n",
    "print(\"[+] Preparing Train-Test data\")\n",
    "all_samples = vstack((benign_tfidf,malicious_tfidf))\n",
    "all_classes = []\n",
    "all_classes.extend(benign_classifications)\n",
    "all_classes.extend(malicious_classifications)\n",
    "\n",
    "kfold = 5\n",
    "print(f\"[+] {kfold} Fold Cross Validation Random Forest Classifier\")\n",
    "random_forest = RandomForestClassifier(warm_start=True)\n",
    "scores = cross_validate(random_forest, all_samples, all_classes, scoring = {\"accuracy\": make_scorer(accuracy_score), \n",
    "                        \"precision\" : make_scorer(precision_score), \"recall\": make_scorer(recall_score)}, cv=kfold)\n",
    "\n",
    "print(\"[+] Evaluating model\")\n",
    "\n",
    "accuracy = scores[\"test_accuracy\"].mean()\n",
    "precision = scores[\"test_precision\"].mean()\n",
    "recall = scores[\"test_recall\"].mean()\n",
    "print(f\"\\t- Accuracy = {accuracy}\") # (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"\\t- Precision = {precision}\") # tp / (tp + fp)\n",
    "print(f\"\\t- Recall = {recall}\") # tp / (tp+fn)\n",
    "\n",
    "random_forest.fit(all_samples, all_classes)\n",
    "\n",
    "# save model weights\n",
    "pickle.dump(random_forest, open(\"RandomForest_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"tfidfvectorizer.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "     type: \"Punctuator\",\n",
       "     value: \"<\"\n",
       " }, {\n",
       "     type: \"Identifier\",\n",
       "     value: \"script\"\n",
       " }, {\n",
       "     type: \"Punctuator\",\n",
       "     value: \">\"\n",
       " }, {\n",
       "     type: \"Punctuator\",\n",
       "     value: \"<\"\n",
       " }, {\n",
       "     type: \"Punctuator\",\n",
       "     value: \"/\"\n",
       " }, {\n",
       "     type: \"Identifier\",\n",
       "     value: \"script\"\n",
       " }, {\n",
       "     type: \"Punctuator\",\n",
       "     value: \">\"\n",
       " }]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= '<script>\\n</script>'\n",
    "esprima.tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.ensemble._forest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-60461b596997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.js'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mRandomforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\RandomForest_model.sav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[0mtfidf_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\tfidfvectorizer.sav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.ensemble._forest'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import js_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import esprima\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import feature_extraction\n",
    "from feature_extraction import tfidf_extractor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "# 0 - benign, 1 - malicious\n",
    "\n",
    "staticscripts = []\n",
    "\n",
    "def extract_script_from_html(html: str):\n",
    "    global staticscripts\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    scripts = []\n",
    "    for script in soup.find_all(\"script\"):\n",
    "        scripts.append(script)\n",
    "    staticscripts = scripts\n",
    "    return scripts\n",
    "\n",
    "def tokenize_html(html: str):\n",
    "    scripts_tokenize = ''\n",
    "    for script in extract_script_from_html(html):\n",
    "        print(script)\n",
    "        scripts_tokenize += esprima.tokenize(str(script))\n",
    "        print(scripts_tokenize)\n",
    "    return scripts_tokenize\n",
    "\n",
    "def tokenize(file_path: str, js_file=True):\n",
    "    try:\n",
    "        with open(file_path, \"r\", errors='surrogateescape') as f:\n",
    "            if js_file:\n",
    "                return esprima.tokenize(f.read())\n",
    "            else:\n",
    "                return tokenize_html(f.read())\n",
    "    except:\n",
    "        return []\n",
    "file_path = sys.argv[1]\n",
    "\n",
    "file = tokenize(file_path,file_path.endswith('.js'))\n",
    "Randomforest = pickle.load(open(r\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\RandomForest_model.sav\", \"rb\"))\n",
    "tfidf_vector = pickle.load(open(r\"C:\\Users\\edenm\\Documents\\GitHub\\DS_Microsoft_Hackaton\\tfidfvectorizer.sav\", \"rb\"))\n",
    "\n",
    "flat = \" \".join([\"{0}_{1}\".format(token.type,token.value.replace(' ','-')) for token in file])\n",
    "\n",
    "tfidf_score = tfidf_vector.transform([flat])\n",
    "predict = Randomforest.predict(tfidf_score)\n",
    "\n",
    "output = {\"Malicious\": bool(predict), \"Event\": staticscripts, \"Confidence\":Randomforest.predict_proba(tfidf_score).max(), \"Multicase\": \"-----------\"}\n",
    "#output_file = open(\"output_file.json\", \"w\")\n",
    "#pickle.dump(output, output_file)\n",
    "with open('output_file.json', 'w') as f:\n",
    "    json.dump(output, f)\n",
    "f.close()\n",
    "#output_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
